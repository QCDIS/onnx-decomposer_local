# ONNX Decomposer (Local)

**ONNX Decomposer for the Deployment of Machine Learning Models on Serverless**

This project is divided into two repositories: 
- [onnx-decomposer_local](https://github.com/adrien-glg/onnx-decomposer_local) for local inference
- [onnx-decomposer_aws](https://github.com/adrien-glg/onnx-decomposer_aws) for Serverless inference on AWS

This is part of a Software Engineering Master's Thesis, carried out at the University of Amsterdam (UvA). You will find all the details about this project in the [Thesis Report](https://dspace.uba.uva.nl/bitstreams/7b31ef71-cf20-4396-9c27-56f86f0b7f55/download).

## Thesis Abstract

In recent years, Serverless computing has emerged as a persuasive paradigm aiming to reshape the cloud
computing landscape considerably. Serverless offers a scalable and cost-effective deployment model where
users can run applications without the need to manage or provision servers. The underlying infrastructure
is entirely abstracted and has the ability to scale automatically in a flexible manner, while the users are
charged exclusively for the resources they use. In parallel, we have witnessed a surge in the adoption
of Artificial Intelligence and Machine Learning (ML) technologies in various application domains. Since
Serverless architectures are not tailored to address the unique challenges posed by resource-intensive
jobs, combining ML with Serverless proves to be a complex undertaking.    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this thesis, we propose a solution for deploying ML models on Serverless platforms, specifically for
inference jobs. Our model-agnostic approach is based on a flexible decomposition of such models into sub-models,
referred to as slices, and the execution of inferences in a workflow of Serverless functions. We rely
on conducting a thorough investigation of the limitations affecting the most popular Serverless platforms
on the market and devising strategies to overcome them. Our experimental evaluations are performed
on AWS, considering the ONNX open source format for ML model representation. Our results show
that our decomposition method enables running ML inference on Serverless, regardless of the model size,
benefiting from the high scalability of this architecture while lowering the strain on computing resources
such as required runtime memory.

## Initial Configuration

If needed, perform the preliminary steps first: [README_preliminaries.md](README_preliminaries.md)

If not already done, configure the following files as needed:
- `general_config.ini`
- `projects/<projectname>/<projectname>_config.ini`
- `projects/<projectname>/<projectname>_steps.py`

To be safe, it is recommended to use a Python Virtual Environment (venv): `python3 -m venv venv`        
You can activate the virtual environment with `source venv/bin/activate`            
You can deactivate the virtual environment with `deactivate`

From the `root` of the project (`onnx-decomposer_local` folder), run the following commands:        
Install the requirements:
```bash
pip install -r requirements.txt
```

Configure the Python path:
```bash
export PYTHONPATH=$PYTHONPATH:"$PWD":"$PWD/src"
```

## Decomposition and inference

Configure the file `general_config.ini` with the project name and the desired number of slices for the decomposition.

Move to the source folder:
```bash
cd src
```

There are multiple modes to perform different actions:
- `basic` mode: Performs a decomposition in slices, and runs an inference.
- `decomposition` mode: Only performs a decomposition in slices.
- `inference` mode: Only runs an inference (it is required to perform a decomposition beforehand). 

To perform an action, run the following command with the desired mode as argument:
```bash
python3 main.py <mode>
```

## Conformity checks

There is a mode to perform each conformity check:
- **Analyze payload size per layer:** `payload_per_layer` mode.      
Prints the size of the payload generated by each layer.
- **Check temporary storage size violation:** `max_slice_size` mode.      
  Prints the size of the heaviest slice.
- **Check payload size violation:** `payload_per_slice` mode.       
  Prints the size of the virtual payload generated by each
  slice.
- **Check memory limit violation:** `memory` mode.          
Prints the memory usage for the inference.

*NB: To perform the conformity check **Check package size violation**, please see the following GitHub repository: 
[onnx-decomposer_aws](https://github.com/adrien-glg/onnx-decomposer_aws).*

To perform a conformity check, run the following command with the desired mode as argument:
```bash
python3 main.py <mode>
```

## AWS-related steps

To go through these steps, you will need a functional AWS account. 
For simple workloads and models, a free tier account is sufficient.         

At the end of the execution, ONNX slices are uploaded to AWS S3 (AWS Cloud Storage).            
For this step to complete, you have to create an S3 bucket.        
Once the S3 bucket is created, configure the file `<projectname>_config.ini` with your AWS region
and the name of the S3 bucket.

## References

This project includes code and content from the following sources:
- [tensorflow-onnx](https://github.com/onnx/tensorflow-onnx/)
